27/06/24
Inizio i lavori per la tesi di laurea triennale di Ingegneria e scienze informatiche
- creazione della repo
- consultazione esperienze e esempi di tesi triennali
- inizio studio dei concetti di visione artificiale applicati alla jvm 
- creazione progetto di test per la visione artificiale
- scoperta degli ArUco markers
- creazione applicazione java base che visualizza ciò che la webcam vede

fonti:
https://www.youtube.com/watch?v=iReJXUeLb20
https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html
https://github.com/bytedeco/javacv

28/06/24
continuo sviluppo applicazione di test
- studio libreria specifica di visione artificiale per java Boofcv

29/06/24
continuo sviluppo applicazione di test
- test su opencv
- sviluppo prima app che riconosce qr code
- sviluppo prima app che riconosce aruco markers (non testata)

01/07/24
- test applicazione sviluppata in data 29/06 e successivi miglioramenti
- utilizzo di marker aruco stampati, dimensioni 4x4(50, 100, 250, 1000)
- studio pagina "Detection of aruco markers" di opencv
- studio concetto di posa della camera
- inizio calibrazione camera con aruco board

fonti:
https://chev.me/arucogen/
https://docs.opencv.org/4.x/da/d13/tutorial_aruco_calibration.html

02/07/24
- continuo calibrazione e posa camera
- download librerie opencv
- ho inserito le librerie di opencv all'interno della cartella bin del jdk //non credo che sia necessario ad oggi 27/07/24 non trovo niente di nuovo dentro la cartella bin
- tutto questo per poter utilizzare una classe completa per la calibrazione trovata su gituhub
- divisione funzioni in più classi
- pubblicazione prima release su github e modifica repo pubblica

fonti:
https://sourceforge.net/projects/opencvlibrary/files/4.5.3/

06/07/24
- continuo calibrazione e posa camera
- tentativo sostituzione libreria opencv453 con opencv9
- importazione nuova libreria per utilizzare versione differente di opencv
- grazie a chatgpt ho scoperto 
    
    static {
        // Carica le librerie native di OpenCV
        Loader.load(opencv_java.class);
    }

    con la quale sono riuscito a caricare senza problemi le librerie opencv

- mi sono bloccato sulla funzione calibrateCamera che non riesce a ottenere un'immagine

fonti:
https://www.baeldung.com/java-opencv

08/07/24
- continuo calibrazione e posa camera
- finito di implementare la calibrazione della camera
- inizio sviluppo posa camera

09/07/24
- continuo calibrazione e posa
- sono riuscito a sviluppare un primo sistema di posa ma non funziona benissimo

fonti: https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html

10/07/24
- sto cercando di capire perchè la posa della camera non disegna
    gli assi in maniera corretta
- ho provato a calibrare la camera per più tempo ma non cambia niente
    allora sto provando a calibrarla con più di 4 marker e quindi 
    ho creato una classe con una funzione per generare pagine di marker
- non ho ottenuto alcun risultato modificando tempi e numero di marker,
    credo proprio ci sia un problema di calibrazione e posa

11/07/24
- ho stampato delle tavole di marker aruco per fare dei test
    e cercare di capire perche non funziona la posa e/o la calibrazione

12/07/24
- cerco di capire bene se i parametri della webcam vengono ottenuti correttamente
    e se rispecchiano quelli reali
- sono riuscito a sviluppare un programma python capace di calibrare la camera
- ora cerco di aggiustare la posa per far funzionare tutto il sistema nel suo insieme
- sono riuscito a far andare la posa e la calibrazione assieme, il problema è
    che la calibrazione al momento va solo grazie al codice python, mentre la
    posa funziona solo attraverso funzioni deprecate, con le funzioni odierne 
    non riesco a farlo funzionare

fonti: https://www.geeksforgeeks.org/camera-calibration-with-python-opencv/

13/07/24
- sto cercando di capire meglio il funzionamento dei metodi per la posa, 
    così da cercare di utilizzare solo metodi attuali e non quelli deprecati
- sono riuscito a sviluppare una classe per la posa della camera funzionante
- non sono ancora riuscito a creare un metodo in java per poter calibrare la camera
    correttamente, al momento però ho impostato che la calibrazione camera prende 
    input dallo storage
- momentaneamente nella classe App sono presenti due parametri hardcoded per poter
    utilizzare i valori della camera restituiti dall'applicazione in python

18/07/24
- inizio calcolo precisione della rilevazione e velocità della rilevazione
- sto cercando di capire perchè la variabile markerLength che indica la lunghezza
    dei marker influenzi di poco la posa anche se sballata di molto
- sto cercando di capire perchè i marker appaiono alla distanza sbagliata

19/07/24
- test con python per capire se i problemi sono causati dal codice java o se
    in generale le funzioni opencv garantiscono poca precisione

20/07/24
- le prove con python stanno dando alcuni problemi, ora sto cercando di capire
    perchè la distanza della camera dal marker viene calcolata male
- in teoria calibrazione e posa in python sono corrette ed infatti anche l'errore
    di riproiezione e molto basso, adesso provo a convertire in java
- sono riuscito a riportare le modifiche anche su java di conseguenza ora e possibile
    vedere il tempo medio di elaborazione per frame e l'errore di riproiezione medio
- continuo ad avere problemi con la stima della distanza che in base alla dimensione del 
    marker e sempre doppia rispetto al valore che ci si aspetterebbe

fonti: 
https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html
https://docs.opencv.org/4.10.0/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d
https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html

23/07/24
- alcuni test per verificare l'affidabilità del sistema e qualche aggiustamento
    per far funzionare la calibrazione in java
- ho trovato in rete fondamenti che riportano la maggiore precisione delle scacchiere
    per calibrare la camera, a discapito della versatilità offerta dai marker aruco
    ho deciso quindi di utilizzare le schacchiere semplici dato che posso ottenere immagini
    di scacchiere di buona qualità una volta ed eseguire una tantum la calibrazione, 
    non mi serve che sia dinamica e flessibile dato che la eseguo solo la prima volta

fonti: https://webthesis.biblio.polito.it/secure/29489/1/tesi.pdf

24/07/24
- le calibrazione della camera effettuate dal codice python e java restituiscono risultati
    leggermente diversi, sto quindi provando ad utilizzare la libreria opencv 4.10 anche in
    java dato che python utilizza questa versione mentre al momento java utilizza la 4.5.5
- alla fine ho deciso di utilizzare le librerie wrapper offerte da javacv, aggiornate al momento alla loro versione
    1.5.10 che corrispondono a opencv 4.9.0
- sono riuscito ad aggiornare il codice con javacv 1.5.10 cioe opencv 4.9.0 ma ho avuto dei grandi
    problemi nel capire che molte delle classi all'interno del package Aruco sono state spostate
    all'interno del package Objdetect come ad esempio tutto ciò che riguarda i Dictionary, GridBoard
    e in parte anche la rilevazione dei marker

fonti: 
https://github.com/bytedeco/javacv
http://bytedeco.org/javacpp-presets/opencv/apidocs/    documentation for javacv 1.5.10 -> opencv 4.9.0
https://docs.opencv.org/4.9.0/javadoc/                 documentation for opencv 4.9.0

25/07/24
- ho revisionato un po il codice e ho effettuato alcuni commit importanti sul git che contenevano
    il primo codice completo per calibrare e calcolare la posa dei marker.
    Ora con una calibrazione più precisa, anche il calcolo della distanza effettuato mediante l'uso della
    norma del vettore di traslazione restituisce valori quasi impeccabili con un errore nell'ordine dei cm
    anche se è necessario correggere tale distanza dividendola per una costante di valore 2.5
- ho provato ad effettuare alcune operazioni di thresholding (soglia/sogliatura) dell'immagine per capire
    se una rilevazione più fine mi garantisse risultati migliori ma per ora sembra cambiare poco-niente
- ho continuato a revisionare il codice (specialmente quello della posa) per prendere maggiore coscienza di
    come esso lavori e come i singoli metodi siano strutturati
- sto cercando di capire e interpretare i vettori tvecs e rvecs per poter creare un
    piano cartesiano su cui veder muovere i miei marker.

26/07/24
- sto cercando di capire se esiste una libreria migliore per catturare le immagini dell webcam dato
    che quella che sto usando ora sembra avere una qualità piuttosto bassa
- considerando che la libreria che uso è quella di opencv penso che continuerò ad usare questa dato che
    offre molte funzioni e me utili e non ho trovato altre librerie migliori sotto l'aspetto della risoluzione
- ho scavato a fondo per capire se la calibrazione fosse corretta (sempre per il fatto che la distanza viene
    calcolata male) ma prendendo un codice python che calibra e rimuove la distorsione dalla camera sono 
    arrivato alla conclusione che la calibrazione è ottima
- continuo a cercare di comprendere al meglio tutte le funzioni di javacv utilizzate

fonti: https://www.youtube.com/watch?v=H5qbRTikxI4

27/07/24
- continuo comprensione metodi opencv/javacv approfondimento su immagini non distorte e utilizzo di 
    esse per effettuare la posa della camera
- prova a calibrare la camera con immagini che non siano solo al centro del campo visivo
    (HO PROVATO MA NON è CAMBIATO NULLA ANZI è QUASI PEGGIORATO SECONDO ME PER 
    ABBASSARE L'ERRORE DI CALIBRAZIONE SOTTO 0.45 E NECESSARIO CAMBIARE WEBCAM)
- mi sono accorto che con marker 4x4 alcune volte il sistema rileva marker anche in oggetti di forma quadrata
    che però non anno niente a che fare con i marker, da valutare un possibile passaggio a dizionari
    5x5 o addirittura 6x6
- sto anche provando a far partire le coordinate della camera da valori specifici come (0;0)
    e a rendere questa modifica permanente ad ogni distanza della camera dai marker

29/07/24
- sto cercando di comprendere perchè le coordinate dei marker rilevati dalla camera 
    hanno sempre x e y negativi
- mi sono accorto che il problema delle coordinate era causato dal fatto che l'immagine catturata dalla
    classe VideoCapture aveva le dimensioni standard di 600x400 ma la risoluzione della camera era in
    realta di 1920x1080 di conseguenza tutta una parte di cattura veniva croppata e non venivano calcolati
    i valori corretti di tvec e rvec, sto ancora cercando di capire se il problema e del tutto risolto o 
    solo in parte ma è già un passo avanti
- ho risolto tutti i problemi riguardanti la posa, aggiustando la dimensione della finestra catturata 
    (in questo caso 1920x1080) tutti i valori di tvec e rvec sono precisi e anche la distanza ora viene 
    calcolata in maniera impeccabile, ho aggiunto anche una enum che mi permette di selezionare in automatico
    tra le principali risoluzioni supportate dalla camera partendo dalla più alta.

fonti: 
https://answers.opencv.org/question/189506/understanding-the-result-of-camera-calibration-and-its-units/
https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html

30/07/24
- lavoro su alcuni miglioramenti in termini di efficienza e velocità del ciclo while nel quale 
    vengono rilevati i marker e calcolata la posa
- mi sono accorto che posso usare alcune strategie per migliorare la velocita di cattura delle immagini
    come ad esempio evitare di usare il metodo undistort per analizzare le immagini, se la camera non 
    presenta distorsioni molto elevate conviene non chiamare tale funzione in quanto molto dispendiosa

31/07/24
- lavoro sulla parallelizzazione del codice
    alla fine ho deciso di non parallelizzare dato che il quadagno era minimo
- ho provato a fare dei test utilizzando come input un video preregistrato con la camera del mio cellulare
    così da poter testare una camera a 60fps ma comunque sembra che vi sia una perdita significativa del 
    marker che spesso durante i movimenti di camera non viene rilevato per circa 30/35 millisecondi fino
    ad arrivare a perdite maggiori fino a 500 millisecondi
- ho trovato un'articolo (quello nelle fonti) che spiega come velocizzare e rendere più efficiente il processo
    di rilevazione dei marker e quindi sto provando ad attuarlo per vedere se ottengo vantaggi significativi
- l'articolo parlava di possibili soluzioni molto a livello teorico senza scendere troppo nel pratico, e stato
    però utile per capire che potrebbe essere un problema di motion blur dovuto allo scarso frame rate della
    camera e in generale all'hardware su cui il software viene fatto girare
- ho scoperto che marker più piccoli portano via meno tempo per la rilevazione a discapito però della precisione

fonti: http://andrewd.ces.clemson.edu/courses/cpsc482/papers/RMM18_speededAruco.pdf

01/08/24
- sto continuando a fare dei test per capire dove si può ottimizzare l'algoritmo e mi sono accorto che solo
    la funzione che disegna gli assi e le posizioni sull'immagine di output impiega 30ms per essere eseguito,
    quindi può essere rimosso/commentato per il momento
- sono riuscito a ridurre all'osso tutti i tempi di elaborazione di immagini e posa, riducendo la risoluzione
    dell'immagine utilizzata da detectMarkers e rimuovendo la fase di disegno degli assi sui marker, adesso avendo
    la webcam che massimo registra 30fps e avendo un tempo di elaborazione di circa 15ms tra rilevazione e posa
    i restanti 15ms vengono sprecati dalla camera in quanto non riesce a leggere più veloce di così essendo a 30fps
- il codice al momento e parecchio sporco e va ripulito da molti test, inoltre vanno aggiunti alcuni paramteri di
    rilevazione come minMarkerDistanceRate min/maxMarkerPerimeterRate e magari giocare un po sul thresholding
    per evitare di leggere più marker del dovuto e aggiustare ogni aspetto della rilevazione
- bisogna stare attenti a non diminuire troppo la risoluzione dell'immagine usata per fare il detect dei marker
    altrimenti i pixel utilizzati non sono sufficienti a rilevare tutti i marker nell'immagine
- fai altri test sul loss, così da capire se puo essere sufficiente così o magari testa i 60 frame con i video del
    telefono

03/08/24
- ho continuato a fare test per valutare le prestazioni del sistema e mi sono accorto che le azioni che impiegano
    più tempo per essere eseguite sono proprio tutte quelle operazioni che servono a stampare gli assi sul frame, 
    disegnare il frame sul canvas o ridimensionare la finestra per inserirla in modo più gradevole nel canvas,
    questo significa che in una possibile implementazione del sistema, rimuovendo tutte queste funzioni "superflue"
    in quanto utili soltanto ad avere un riscontro grafico di ciò che la camera vede, il sistema può velocizzarsi di molto
    lavorando anche a frame più alti (60fps) permettendo quindi una diminuzione del motion blur e in generale una 
    rilevazione e posa migliori
- tieni a mente che per i tempi di rilevazione dei frame ci sono tre parametri principali:
    il tempo di rilevazione dei marker non cambia di molto con l'aumentare dei marker ma è già di per se abbastanza alto
     è possibile abbassare questo tempo grazie a risoluzioni più basse

    il tempo di posa dei marker, tendenzialmente basso ma aumenta molto all'aumentare dei marker, lo si può abbassare 
     parecchio evitando di stampare sul frame i vari assi e la posizione dei marker

    il tempo per ridimensionare stampare sul canvas il frame

    il tempo per ottenere un nuovo frame
- sarebbe importante avere una webcam 60fps con esposizione regolabile in modo da poter ridurre al minimo il motion blur

05/08/24
- continuo di test con video preregistrati e webcam, la reattività del sistema sembra buona, con tempi in ms molto bassi
    per quanto riguarda i video preregistrati e un po' più alti nel caso di webcam, questo forse dovuto al fatto che la 
    webcam ha una qualità inferiore rispetto a quella dello smartphone, sperando in una webcam di qualità questi problemi
    dovrebbero risolversi, inoltre la perdità di marker è ancora piuttosto presente anche se penso possa essere mitigata
    di molto usando webcam con frame rate più alto e personalizzazione dei parametri di esposizione ecc...
- in realtà la velocità reattività maggiore dei video preregistrati è data dal fatto che avendoli passati sul pc tramite
    whatsapp la loro qualità è molto calata, quindi avendo frame più piccoli e meno pesanti, sono più facili da ottenere
- anche l'aiuto di hardware più potente credo possa migliorare di molto le prestazioni delle funzioni di opencv

06/07/24
- piccoli test su parametri per la rilevazione dei marker come minMarkerDistanceRate min/maxMarkerPerimeterRate, 
    niente di particolarmente importante, e soprattutto dopo alcuni test non ho ritenuto necessari questi parametri quindi li ho 
    rimossi
- ho continuato a cercare di capire perchè alcune volte la fase di cattura dell'immagine impiega molto tempo 
    e sono giunto alla conclusione che il problema principale è dato dall'elevata qualità dell'immagine da
    catturare e dal suo bitrate, il problema può essere attenuato come già faccio abbassando la qualità dell'immagine
    di un certo valore anche se dovrei abbassarla già in fase di cattura e non solo in fase di rilevazione marker così
    da ridurre sia i tempi di cattura che quelli di rilevazione e non solo questi ultimi
- sono finalmente riuscito a abbassare il tempo di esposizione e quindi adesso il motion blur e nettamente migliorato e 
    penso di poter far muovere i robot a più di 1 m/s, inoltre sono riuscito a farlo su una camera a 30fps quindi 
    probabilmente l'unico vincolo è rappresentato dall'illuminazione della stanza che deve consentire immagini ben visibili
    anche con tempi di esposizione brevi

07/08/24
- qualche test sui valori di rotazione dei marker

09/08/24
- ho sistemato il codice della repo e ho iniziato a guardare il robottino
- dovrei fare che la calibrazione scrive i dati della camera su un file e ogni volta invece di eseguire la calibrazione controllo
    se il file esiste e in caso prendo i dato dal file

21/08/24
- sto cercando di capire come poter programmare il robot, ho capito che il "cervello" del robot è composto da un M5 CORE,
    e che può essere programmato via usb o via wifi tramite un programma chiamato UIFlow anche se ho trovato un'estensione di
    vs code che però al momento non funziona

- ho flashato il codice base del M5 CORE e utilizzando UIFlow sono riuscito a programmare semplici cose, suoni, colore dello 
    sfondo ecc... 
    tutto questo tramite la funzione wifi, ora sto provando attraverso la funzionalità usb a utilizzare l'estensione chiamata:
    vscode-m5stack-mpy per vscode, ho avuto alcuni problemi con questa estensione perchè non mi permetteva di selezionare la porta,
    come dice infatti la documentazione, una volta installata dovrebbe comparire un pulsante "Add M5Stack" nella barra di stato in
    fondo allo schermo ma questo non accadeva, ho risolto con i passaggi a questo link, utilizzando alcuni comandi npm su terminale:
    https://github.com/curdeveryday/vscode-m5stack-mpy/issues/13#issuecomment-1895764675
    una volta lanciati tali comandi e riavviato vscode il pulsante e comparso e funziona
    
    API KEY: 67FA2E42

fonti: 
https://docs.m5stack.com/en/uiflow/uiflow_web           Link per scaricare il burner 
https://docs.m5stack.com/en/uiflow/m5core/program       Link del tutorial per fleshare il firmware base per M5 CORE

22/08/24
- continuo a cercare di programmare il robot, ora sto riuscendo a farlo comunicare con vscode grazie all'estensione trovata ieri
- il problema e che non so come utilizzare
  le funzioni del lidarbot perchè l'm5 core è una cosa ma come si vede dalle immagini in questa pagina: 
  https://docs.m5stack.com/en/app/lidarbot quest'ultimo comunica con una scheda che comanda effettivamente tutte componenti del 
  robot di conseguenza non so come controllarlo
- adesso provo con il codice della repo: https://github.com/m5stack/Applications-LidarBot e arduino a lanciare e modificare
    tale codice per vedere se funziona
- con arduino so avendo molti problemi, molto lento a verificare il codice e nonostante tutti i passaggi ancora non commpila
    correttamente, sto provando quindi con platformIO in vscode
- Innanzitutto ho importato tramite l'interfaccia di PlatformIO le librerie M5Unified e M5Dial (forse Dial non serviva), poi
    ho dovuto modificare tutti gli include di M5Stack con M5Unified perché ormai Stack è deprecato e quindi non è più incluso nelle
    librerie, dopodiché ho dovuto modificare alcune funzioni che in Unified erano leggermente diverse
    Per il momento il codice compila e può essere flashato sul robot, il tutto sembra andare, adesso non resta che capire il codice e
    smanettarci un po'

24/08/24
- piccolo test per verificare il funzionamento dell'applicazione di rilevazione e alcuni test con il robot e la comprensione del codice

26/08/24
- continuo analisi codice robot
- sto provando ad inserire l'immagine del marker nello schemro dell' M5Core
- sono riuscito a inserire l'immagine, va codificata in esadecimale con 16 bit per pixel ed è importante che l'immagine sia 320x240 e che
    per ogni nel file c siano scritti tutti i byte per ogni pixel es: 0X4F e non che un elemento dell'array nel file c rappresenti tutti
    i byte di un pixel es: 0X67FF
    è anche importante che l'array nel file c abbia dimensione di 153600
- infine ho continuato con alcuni test per capire perchè il lidar non stampasse su schermo le distanze dagli oggetti
- c'è un errore secondo il quale quando entro nel while del getData la linea di clock scl e data sda di i2c sono collegate allo stesso
    pin (indaga)

fonti: 
https://docs.m5stack.com/en/arduino/m5core/lcd      Documentazione librerie m5core
https://notisrac.github.io/FileToCArray/            Libreria per convertire immagini in array C, (ricordati di spuntare la casella "Separate bytes for pixels)

27/08/24
- continuo analisi funzioni lidar
- ho ricreato il progetto di platformIO perche avevo fatto alcuni danni ora e anche pushato sulla repo nella sua versione base e funzionante

28/08/24
- sto cercando di sistemare lo scambio dati sul protocollo i2c che pare non funzionare, ho visto che l'indirizzo corretto e 0x75 attraverso uno scanner
- mi sono accorto che io sono in possesso del lidarBot2 e provando ad utilizzare il codice per il LidarBot-X2 sembra funzionare meglio, 
    quindi ho creato un nuovo progetto di platformIO per fare alcune prove e almeno il lidar sembra andare
- ho scaricato anche sul telecomando il codice del lidarbot-X2 e adesso ho  solo una modalità di movimento "remote" che però visualizza sia 
    sul telecomando che sul robot i dati del Lidar, procedo con alcuni test per l'applicazione di visione artificiale, velocità del robot, luce
    necessaria ecc...
- provo a far comunicare il codice java con il robot
- sono riuscito a creare un piccolo codice lato esp32 e lato java che permette di assumere il ruolo di server al robot e il ruolo di client al programma
    java, così da poter scambare i dati tra i due (in questo modo posso far inviare i dati da java a esp32 per comandare il robot)
- attento perchè nella classe espHttpServer viene avviato il wifi come ap (access point) potrebbe quindi creare problemi con quello che voglio fare io

29/08/24
- ho fatto alcuni test con http e espnow entrambi accesi ma l'http da in qualche modo fastidio all'espnow e quindi non gli permette di funzionare
    correttamente, per il momento quindi commentero le parti relative all'espnow dato che mi serve testare la comunicazione http java-esp
- sono risuscito a capire che i motori sono in questo ordine: sinistro anteriore A, destro anteriore B, sinistro posteriore C, destro posteriore D
- ho continuato con alcuni test, il tutto sembra funzionare anche se non mi è molto chiaro perchè ma a volte se giro il robot a sinistra (o a destra)
    per un dato lasso di tempo ad una data intensità il risultato è leggermente diverso e quindi e difficile far ruotare il robot di angoli predefiniti
    in maniera precisa
- ho anche capito che la funzione ControlWheel di lidarcar funziona così: 
    X > 0 gira a destra, X < 0 gira a sinistra, Y > 0 vai avanti, Y < 0 vai indietro 
- ho iniziato a scriver la classe RobotScreenSaver che si occupera di calcolare la posa dei marker (codice copiato da CameraPose), dopodichè stabilirà
    una sorta di perimetro fatto dai marker (1,2,3,4) e permettera al robot (indicato con marker 0) di muoversi all'interno di tale perimetro un po'
    come se fosse lo screen saver di windows
- sono riuscito a far muovere e far controllare al robot lo spazio che lo circonda anche se ancora non funziona benissimo e alcune volte si gira male

30/08/24
- ho continuato con i test per il RobotScreenSaver e sono riuscito a farlo andare più o meno, sono sorti alcuni problemi:
    innanzitutto il marker 0, quello usato per il robot è 5mm più grande rispetto agli altri (75mm - 70mm) inoltre essendo posizionato sul robot è anche più
    in alto, questo fa si che il marker 0 risulti più vicino alla camera rispetto ai marker degli angoli del perimetro e questo alcune volte causa un
    comportamento anomalo nel robot che prosegue per qualche cm in più verso il bordo prima di girare oltrepassando i marker sul pavimento.
- ho provato a risolvere il problema che faceva si che il robot una volta raggiunto un bordo continuasse a ruotare su se stesso non riuscendo
    più a tornare all'interno del perimetro semplicemente facendolo muovere avanti per circa 1secondo subito dopo averlo fatto girare, 
    non ho avuto modo di provare approfonditamente questa modifica anche se sembrava funzionare perchè si sono scaricate le batterie del robot
    e non si caricano correttamente
- appena avrai risolto devi provare a sistemare il codice (uniformando dove c'è da girare a sinistra e dove a destra) cosi da ridurre gli else if
    e dovrai anche dare una sistemata al codice per fare si che sia più snello e non la copia carbone del codice di CameraPose

31/08/24
- ho diviso il codice di CameraPose.calcPose() in più funzioni per rendere il codice più leggibile e riutilizzabile, inoltre ho creato
    altre due funzioni per ottenere la posa della camera di un singolo frame e ritornare le coordinate dei marker, un metodo richiede che gli sia passata
    la camera dalla quale ottenere le immagini (metodo più veloce), l'altro invece crea tutto al suo interno ogni volta (più lento infatti ogni volta per
    aprire la camera impiega circa 2-3 secondi)